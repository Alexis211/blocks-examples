#!/usr/bin/env python
"""Encoder-Decoder with search for machine translation.

In this demo, encoder-decoder architecture with attention mechanism
is used for machine translation. The attention mechanism is implemented
according to [BCB]_. The training data used is WMT15 Czech to English
corpus, which you have to download and put to your 'datadir' in the
config file. You might need to tokenize, clean and shuffle your data.
Dictionaries for source and target languages can be generated by using
GroundHog preprocessing pipeline.
https://github.com/lisa-groundhog/GroundHog/tree/master/experiments/
nmt#data-preparation

.. [BCB] Dzmitry Bahdanau, Kyunghyun Cho and Yoshua Bengio. Neural
   Machine Translation by Jointly Learning to Align and Translate.
"""

import argparse
import importlib
import logging
import pprint
import os
import sys

from operator import mul

from collections import OrderedDict, Counter
from theano import tensor
from toolz import merge

from blocks.algorithms import (GradientDescent, StepClipping,
                               CompositeRule)
from blocks.extensions import FinishAfter, Printing
from blocks.extensions.monitoring import TrainingDataMonitoring
from blocks.filter import VariableFilter
from blocks.graph import ComputationGraph, apply_noise, apply_dropout
from blocks.initialization import IsotropicGaussian, Orthogonal, Constant
from blocks.main_loop import MainLoop
from blocks.model import Model
from blocks.select import Selector
from blocks.roles import has_roles

from model import CustomizedGradientDescent

from model.clustering import ClusteredSoftmaxEmitter, ReclusterExtension

try:
    from blocks.extras.extensions.plot import Plot
    bokeh_available = True
except ImportError:
    bokeh_available = False


from sampling import BleuValidator, Sampler

from saveload import CheckpointNMT, LoadNMT


logger = logging.getLogger(__name__)


def main(config, data_stream, bokeh=False):

    # Create Theano variables
    logger.info('Creating theano variables')
    source_sentence = tensor.lmatrix('source')
    source_sentence_mask = tensor.matrix('source_mask')
    target_sentence = tensor.lmatrix('target')
    target_sentence_mask = tensor.matrix('target_mask')

    # Construct model
    logger.info('Building RNN encoder-decoder')
    encoder = config.Encoder(vocab_size=config.src_vocab_size,
                             embedding_dim=config.enc_embed,
                             state_dim=config.enc_nhids)

    if hasattr(config, 'num_clusters'):
        clustering_args = {}
        clustering_args['num_clusters'] = config.num_clusters
        clustering_args['cluster_max_size'] = config.cluster_max_size
        clustering_args['emit_k_best_clusters'] = config.emit_k_best_clusters
        clustering_args['cost_k_best_clusters'] = config.cost_k_best_clusters
        clustering_args['mips_to_mcss_params'] = config.mips_to_mcss_params
        decoder_extra_args = {'clustering_args': clustering_args}
    else:
        decoder_extra_args = {}

    decoder = config.Decoder(vocab_size=config.trg_vocab_size,
                             embedding_dim=config.dec_embed,
                             state_dim=config.dec_nhids,
                             representation_dim=config.enc_nhids * 2,
                             **decoder_extra_args)
    cost = decoder.cost(
        encoder.apply(source_sentence, source_sentence_mask),
        source_sentence_mask, target_sentence, target_sentence_mask)

    logger.info('Creating computational graph')
    cg = ComputationGraph(cost)

    # Initialize model
    logger.info('Initializing model')
    encoder.weights_init = decoder.weights_init = IsotropicGaussian(
        config.weight_scale)
    encoder.biases_init = decoder.biases_init = Constant(0)
    encoder.push_initialization_config()
    decoder.push_initialization_config()
    encoder.bidir.prototype.weights_init = Orthogonal()
    decoder.transition.weights_init = Orthogonal()
    encoder.initialize()
    decoder.initialize()

    # apply dropout for regularization
    if config.dropout < 1.0:
        # dropout is applied to the output of maxout in ghog
        logger.info('Applying dropout')
        dropout_inputs = [x for x in cg.intermediary_variables
                          if x.name == 'maxout_apply_output']
        cg = apply_dropout(cg, dropout_inputs, config.dropout)

    # Apply weight noise for regularization
    if config.weight_noise_ff > 0.0:
        logger.info('Applying weight noise to ff layers')
        enc_params = Selector(encoder.lookup).get_params().values()
        enc_params += Selector(encoder.fwd_fork).get_params().values()
        enc_params += Selector(encoder.back_fork).get_params().values()
        dec_params = Selector(
            decoder.sequence_generator.readout).get_params().values()
        dec_params += Selector(
            decoder.sequence_generator.fork).get_params().values()
        dec_params += Selector(decoder.state_init).get_params().values()
        cg = apply_noise(cg, enc_params+dec_params, config.weight_noise_ff)

    # Print shapes
    enc_dec_param_dict = merge(Selector(encoder).get_parameters(),
                               Selector(decoder).get_parameters())
    shapes = [pvar.get_value().shape
              for _, pvar in enc_dec_param_dict.iteritems()]
    logger.info("Parameter shapes: ")
    for shape, count in Counter(shapes).most_common():
        logger.info('    {:15}: {}'.format(shape, count))
    total_params = sum(reduce(mul, shape, 1) for shape in shapes)
    logger.info("Total number of parameters: {} parameters in {} matrices" \
                .format(total_params, len(shapes)))

    # Print parameter names
    logger.info("Parameter names: ")
    for name, value in enc_dec_param_dict.iteritems():
        logger.info('    {:15}: {}'.format(value.get_value().shape, name))
    logger.info("Total number of parameters: {}"
                .format(len(enc_dec_param_dict)))

    # Set up training model
    logger.info("Building model")
    training_model = Model(cost)
    # not all parameters are in the computation graph, so synchronize them
    training_model._parameter_dict = OrderedDict(enc_dec_param_dict)    

    # Create saving directory if it does not exist
    if not os.path.exists(config.saveto):
        os.makedirs(config.saveto)

    # Set extensions
    logger.info("Initializing extensions")
    extensions = [
        FinishAfter(after_n_batches=config.finish_after),
        TrainingDataMonitoring([cost], every_n_batches=config.train_monitor_freq),
        Printing(every_n_batches=config.train_monitor_freq),
        CheckpointNMT(config.saveto,
                      every_n_batches=config.save_freq)
    ]

    # Set up beam search and sampling computation graphs if necessary
    if config.hook_samples >= 1 or config.bleu_script is not None:
        logger.info("Building sampling model")
        sampling_input = tensor.lmatrix('input')
        sampling_representation = encoder.apply(
            sampling_input, tensor.ones(sampling_input.shape))
        generated = decoder.generate(sampling_input, sampling_representation)
        search_model = Model(generated)
        _, samples = VariableFilter(
            bricks=[decoder.sequence_generator], name="outputs")(
                ComputationGraph(generated[1]))  # generated[1] is next_outputs

    # Reload model if necessary
    if config.reload:
        extensions.append(LoadNMT(config.saveto))

    # If we use a clustering approach, add reclustering extension
    if isinstance(decoder.emitter, ClusteredSoftmaxEmitter):
        extensions.append(ReclusterExtension(
                               emitter=decoder.emitter,
                               max_iters=config.recluster_max_iters,
                               before_training=True,
                               every_n_batches=config.recluster_freq))

    # Add sampling
    if config.hook_samples >= 1:
        logger.info("Building sampler")
        extensions.append(
            Sampler(model=search_model, config=config, data_stream=data_stream.masked_stream,
                    src_vocab=data_stream.src_vocab, trg_vocab=data_stream.trg_vocab,
                    every_n_batches=config.sampling_freq))

    # Add early stopping based on bleu
    if config.bleu_script is not None:
        logger.info("Building bleu validator")
        extensions.append(
            BleuValidator(source_sentence=sampling_input,
                          model=search_model, config=config, data_stream=data_stream.dev_stream,
                          samples=samples,
                          src_vocab=data_stream.src_vocab, trg_vocab=data_stream.trg_vocab,
                          every_n_batches=config.bleu_val_freq))

    # Plot cost in bokeh if necessary
    if bokeh:
        extensions.append(
            Plot(document=config.plot_title,
                 channels=[[cost.name]],
                 every_n_batches=config.train_monitor_freq,
                 server_url='http://eos6:5006'))

    # Set up training algorithm
    logger.info("Initializing training algorithm")
    algorithm = CustomizedGradientDescent(
        cost=cost, parameters=cg.parameters,
        step_rule=CompositeRule([StepClipping(config.step_clipping),
                                 config.step_rule])
    )

    # Initialize main loop
    logger.info("Initializing main loop")
    main_loop = MainLoop(
        model=training_model,
        algorithm=algorithm,
        data_stream=data_stream.masked_stream,
        extensions=extensions
    )

    # Train!
    main_loop.run()





if __name__ == "__main__":
    # Get the arguments
    parser = argparse.ArgumentParser()
    parser.add_argument("--config",  default="default",
                        help="Configuration file to use")
    parser.add_argument("--bokeh",  default=False, action="store_true",
                        help="Use bokeh server for plotting")
    args = parser.parse_args()

    if args.bokeh and not bokeh_available:
        logger.error("Sorry, Bokeh is not installed on your system")
        sys.exit(1)

    # Get configurations for model
    config = importlib.import_module("."+args.config, "config")

    # Run it
    logger.info("Model options:\n{}".format(pprint.pformat(config)))
    data_stream = importlib.import_module(config.stream)
    main(config, data_stream, args.bokeh)
